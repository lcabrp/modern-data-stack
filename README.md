# Modern Data Stack

**The "Boring" Way to Build High-Performance Data Pipelines.**

A serverless, local-first ELT pipeline boilerplate using the **Embedded Data Stack** architecture. Replace complex cloud infrastructure with a lightweight stack that runs entirely on your laptop.

---

## ğŸ› ï¸ The Stack

| Layer | Tool | Role |
|---|---|---|
| **Ingestion** | [dlt](https://dlthub.com/) | Schema-aware API data landing with incremental loading |
| **Storage** | Parquet (local) | Columnar data lake on the filesystem |
| **Transformation** | [DuckDB](https://duckdb.org/) | In-process SQL filtering, joins, and type-casting |
| **Processing** | [Polars](https://pola.rs/) | Multi-threaded DataFrame aggregations |
| **Glue** | [Apache Arrow](https://arrow.apache.org/) | Zero-copy memory sharing between DuckDB â†” Polars |

---

## ğŸ“ Project Structure

```
modern-data-stack/
â”œâ”€â”€ pipeline.py              # Main orchestrator (entry point)
â”œâ”€â”€ ingest.py                # dlt: GitHub API â†’ data/raw/ Parquet
â”œâ”€â”€ transform/
â”‚   â”œâ”€â”€ staging.py           # DuckDB: SQL cleaning â†’ data/staging/
â”‚   â””â”€â”€ marts.py             # Polars: incremental aggregation â†’ data/marts/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Raw API data (Parquet)
â”‚   â”œâ”€â”€ staging/             # Cleaned data (Parquet)
â”‚   â””â”€â”€ marts/               # Aggregated tables (Parquet)
â”œâ”€â”€ pyproject.toml           # Dependencies
â”œâ”€â”€ .env.example             # Environment variable template
â””â”€â”€ README.md
```

---

## ğŸš€ Quickstart

### 1. Prerequisites

- Python â‰¥ 3.10
- [uv](https://docs.astral.sh/uv/) (recommended package manager)

### 2. Install

```bash
# Clone the repo
git clone <your-repo-url> && cd modern-data-stack

# Create a virtual environment & install deps
uv sync
```

### 3. Configure (optional)

```bash
cp .env.example .env
# Edit .env to set GITHUB_TOKEN for higher API rate limits
```

### 4. Run the pipeline

```bash
# Full ELT: ingest â†’ staging â†’ marts
uv run python pipeline.py

# Fetch a specific GitHub org
uv run python pipeline.py --org python

# Skip ingestion, re-run transforms only
uv run python pipeline.py --skip-ingest

# Wider lookback window for incremental processing
uv run python pipeline.py --lookback-days 30
```

---

## ğŸ—ï¸ Architecture â€” Zero-Copy Arrow Flow

```
GitHub API
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Parquet    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  Arrow Table  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  dlt       â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚  DuckDB    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚  Polars    â”‚
â”‚  (ingest)  â”‚  data/raw/   â”‚  (staging) â”‚  zero-copy    â”‚  (marts)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚                             â”‚
                               â–¼                             â–¼
                         data/staging/                  data/marts/
                         repos.parquet              repos_per_language.parquet
                                                    daily_activity.parquet
```

**Key principle**: DuckDB's `.arrow()` method returns an Apache Arrow table. Polars' `pl.from_arrow()` wraps it *without copying memory*. This means the DuckDB â†’ Polars handoff is essentially free regardless of data size.

---

## ğŸ“– What You'll Learn

1. How to land raw API data into a local data lake using **dlt** with incremental loading.
2. How to use **DuckDB** to clean and type-cast data using standard SQL.
3. How to perform **zero-copy** handoffs from DuckDB to **Polars** via Apache Arrow.
4. How to implement the **Lookback** pattern for incremental processing so your pipeline only touches new data.
5. How to structure a Python project like **dbt** (Staging vs. Marts) without needing dbt itself.

---

## ğŸ”¬ Technical Deep-Dive

See [TECHNICAL.md](TECHNICAL.md) for detailed documentation on:
- The zero-copy Arrow handoff mechanism
- The incremental lookback pattern
- Layer responsibilities (staging vs. marts)
- How to extend the pipeline with new sources and marts
- Performance characteristics

---

## License

[MIT](LICENSE)
